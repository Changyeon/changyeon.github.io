---
---

@InProceedings{Cho:2022:PPoPP,
    abbr={PPoPP'22},
    author="Cho, Younghyun and Park, Jiyeon and Negele, Florian and Jo, Changyeon and Gross, Thomas R. and Egger, Bernhard",
    title="Dopia: Online Parallelism Management for Integrated CPU/GPU Architectures",
    booktitle="27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 2022)",
    month="April",
    year="2022",
    publisher="ACM",
    doi="10.1145/3503221.3508421",
    abstract="Recent desktop and mobile processors often integrate CPU and GPU onto the same die. The limited memory bandwidth of these integrated architectures can negatively affect the performance of data-parallel workloads when all computational resources are active. The combination of active CPU and GPU cores achieving the maximum performance depends on a workload’s characteristics, making manual tuning a time-consuming task. Dopia is a fully automated framework that improves the performance of data-parallel workloads by adjusting the Degree Of Parallelism on Integrated Architectures. Dopia transparently analyzes and rewrites OpenCL kernels before executing them with the number of CPU and GPU cores expected to yield the best performance. Evaluated on AMD and Intel integrated processors, Dopia achieves 84\% of the maximum performance attainable by an oracle.",
    pdf="publications/2022.PPoPP.Cho.pdf"
}

@InProceedings{Kim:2021:GECON,
    abbr={GECON'21},
    author="Kim, Hyunik
        and Jo, Changyeon
        and Altmann, J{\"o}rn
        and Egger, Bernhard",
    editor="Tserpes, Konstantinos
        and Altmann, J{\"o}rn
        and Ba{\~{n}}ares, Jos{\'e} {\'A}ngel
        and Agmon Ben-Yehuda, Orna
        and Djemame, Karim
        and Stankovski, Vlado
        and Tuffin, Bruno",
    title="RapidSwap: a Hierarchical Far Memory",
    booktitle="18th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON'21)",
    year="2021",
    publisher="Springer International Publishing",
    address="Cham",
    pages="143--151",
    abstract="As more and more memory-intensive applications are moved into the cloud, data center operators face the challenge of providing sufficient main memory resources while achieving high resource utilization. Solutions to overcome the unsatisfying performance degradation of traditional on-demand paging include memory disaggregation that allows applications to access remote memory or compressing memory pages in local DRAM; however, the former's extended failure domain and the latter's low efficacy limit their broad applicability. This paper presents RapidSwap, a hierarchical far memory manager that exploits the wide availability of phase-change memory (Intel Optane memory) in data centers to achieve quasi-DRAM performance at a significantly lower total cost of ownership (TCO). RapidSwap migrates infrequently accessed data to slower and cheaper devices in a hierarchy of storage devices by tracking applications' memory accesses. Evaluated with several real-world cloud benchmarks, RapidSwap achieves a reduction of 20{\%} in operating cost at minimal performance degradation and is 30{\%} more cost-effective than pure DRAM solutions. The results demonstrate that proper management of new memory technologies can yield significant TCO savings in cloud data centers.",
    isbn="978-3-030-92916-9",
    pdf="publications/2021.GECON.Kim.pdf"
}

@InProceedings{Park:2021:GECON,
    abbr={GECON'21},
    author="Park, Daon
        and Kim, Hyunsoo
        and Cho, Youngsu
        and Jo, Changyeon
        and Egger, Bernhard",
    editor="Tserpes, Konstantinos
        and Altmann, J{\"o}rn
        and Ba{\~{n}}ares, Jos{\'e} {\'A}ngel
        and Agmon Ben-Yehuda, Orna
        and Djemame, Karim
        and Stankovski, Vlado
        and Tuffin, Bruno",
    title="Can VM Live Migration Improve Job Throughput? Evidence from a Real World Cluster Trace",
    booktitle="18th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON'21)",
    year="2021",
    publisher="Springer International Publishing",
    address="Cham",
    pages="17--26",
    abstract="Cloud resource providers are putting more and more emphasis on efficiently management the resources of their data centers to achieve high utilization while minimizing energy consumption. Despite these efforts, an analysis of recent data center traces reveals that the utilization of CPU and memory resources has not improved significantly over the past decade. Resource overcommitment is a promising approach to improve resource utilization, because most workloads show a significant gap between their guaranteed and actually consumed resources. A wrong prediction of the actual usage, however, can lead to a severe performance degradation on overloaded nodes. Combining resource overcommitment with live migration of tasks can alleviate the situation, but its prohibitively high cost has so far prevented a wide adoption. Recent and rapid advancements in networking technology, however, are changing the status quo. With throughputs surpassing 100혻Gb/s in 2021, even large tasks can be migrated within a few seconds. In light of these improvements, we believe it is time to rethink the application of resource overcommitment and live migration to improve data center resource utilization. Based on real-world cluster traces published by Google in 2019, we show that combining resource overcommitment with task live migration can reduce the mean task turnaround time by 16{\%}, demonstrating that further research in this direction is both warranted and promising.",
    isbn="978-3-030-92916-9",
    pdf="publications/2021.GECON.Park.pdf"
}

@InProceedings{Jo:2020:PACT:RackMem,
    abbr={PACT'20},
    author = {Jo, Changyeon and Kim, Hyunik and Geng, Hexiang and Egger, Bernhard},
    title = {RackMem: A Tailored Caching Layer for Rack Scale Computing},
    booktitle = {Proceedings of the 2020 International Conference on Parallel Architectures and Compilation (PACT'20)},
    series = {PACT '20},
    year = {2020},
    isbn = {978-1-4503-8075-1},
    location = {Virtual Event},
    pages = {1--14},
    numpages = {14},
    url = {https://doi.org/10.1145/3410463.3414643},
    doi = {10.1145/3410463.3414643},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = "High-performance computing (HPC) clusters suffer from an overall low memory utilization that is caused by the node-centric memory allocation combined with the variable memory requirements of HPC workloads. The recent provisioning of nodes with terabytes of memory to accommodate workloads with extreme peak memory requirements further exacerbates the problem. Memory disaggregation is viewed as a promising remedy to increase overall resource utilization and enable cost-effective up-scaling and efficient operation of HPC clusters, however, the overhead of demand paging in virtual memory management has so far hindered performant implementations. To overcome these limitations, this work presents RackMem, an efficient implementation of disaggregated memory for rack scale computing. RackMem addresses the shortcomings of Linux’s demand paging algorithm and automatically adapts to the memory access patterns of individual processes to minimize the inherent overhead of remote memory accesses. Evaluated on a cluster with an Infiniband interconnect, RackMem outperforms the state-of-the-art RDMA implementation and Linux’s virtual memory paging by a significant margin. RackMem’s custom demand paging implementation achieves a tail latency that is two orders of magnitude better than that of the Linux kernel. Compared to the state-of-the-art remote paging solution, RackMem achieves a 28\% higher throughput and a 44\% lower tail latency for a wide variety of real-world workloads.",
    pdf="publications/2020.PACT.Jo.pdf"
}

@InProceedings{Jo:2020:GECON,
    abbr={GECON'20},
    author = {Jo, Changyeon and Kim, Hyunik and Egger, Bernhard},
    title = {Instant Virtual Machine Live Migration},
    booktitle="17th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON'20)",
    series = {GECON '20},
    year = {2020},
    publisher = {Springer International Publishing},
    address = {Cham},
    numpages = {14},
    abstract = "Live migration of virtual machines (VMs) is an important tool for data center operators to achieve maintenance, power management, and load balancing. The relatively high cost of live migration makes it difficult to employ live migration for rapid load balancing or power management operations, leaving much of its promised benefits unused. The advance of fast network interconnects has led to the development of distributed shared memory (DSM) that allows a cluster of nodes to utilize remote memory with relatively low overhead. In this paper, we explore VM live migration over DSM. We present and evaluate a novel live migration algorithm on our own DSM implementation. The evaluation of live migrating various VMs executing real-life workloads shows that live migration over DSM can reduce the total migration time by 70\% and the total amount of transferred data by 65\% on average. The absolute average total migration time of only 1.1 seconds demonstrates the potential of live migration over DSM to lead to better load balancing, energy management, and total cost of ownership",
    pdf="publications/2020.GECON.Jo.pdf"
}

@InProceedings{Cho:2020:GECON,
    abbr={GECON'20},
    author = {Cho, Youngsu and Jo, Changyeon and Kim, Hyunik and Egger, Bernhard},
    title = {Towards Economical Live Migration in Data Centers},
    booktitle="17th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON'20)",
    series = {GECON '20},
    year = {2020},
    publisher = {Springer International Publishing},
    address = {Cham},
    numpages = {14},
    abstract = "Live migration of virtual machines (VMs) enables maintenance, load balancing, and power management in data centers. The cost of live migration on several key metrics combined with strict service-level objectives (SLOs), however, typically limits its practical application to situations where the underlying physical host has to undergo maintenance. As a consequence, the potential benefits of live migration with respect to increased resource usage and lower power consumption remain largely untouched. In this paper, we argue that live migration-aware SLOs combined with smart live migration algorithm selection provides an economically viable model for live migration in data centers. Based on a model predicting key parameters of VM live migration, an optimization algorithm selects the live migration technique that is expected to meet client SLOs while at the same time to optimize target metrics given by the data center operator. A comparison with the state-of-the-art shows that the presented guided live migration technique selection achieves significantly fewer SLO violations while, at the same time, minimizing the effect of live migration on the infrastructure",
    pdf="publications/2020.GECON.Cho.pdf"
}

@InProceedings{Jo:2017:SoCC,
    abbr={SoCC'17},
    author = {Jo, Changyeon and Cho, Youngsu and Egger, Bernhard},
    title = {A Machine Learning Approach to Live Migration Modeling},
    booktitle = {ACM Symposium on Cloud Computing (SoCC'17)},
    series = {SoCC'17},
    year = {2017},
    month = {September},
    location = {Santa Clara, CA, USA},
    abstract = "Live migration is one of the key technologies to improve data center utilization, power efficiency, and maintenance. Various live migration algorithms have been proposed; each exhibiting distinct characteristics in terms of completion time, amount of data transferred, virtual machine (VM) downtime, and VM performance degradation. To make matters worse, not only the migration algorithm but also the applications running inside the migrated VM affect the different performance metrics. With service-level agreements and operational constraints in place, choosing the optimal live migration technique has so far been an open question. In this work, we propose an adaptive machine learning-based model that is able to predict with high accuracy the key characteristics of live migration in dependence of the migration algorithm and the workload running inside the VM. We discuss the important input parameters for accurately modeling the target metrics, and describe how to profile them with little overhead. Compared to existing work, we are not only able to model all commonly used migration algorithms but also predict important metrics that have not been considered so far such as the performance degradation of the VM. In a comparison with the state-of-the-art, we show that the proposed model outperforms existing work by a factor 2 to 5.",
    pdf="publications/2017.SoCC.Jo.pdf",
    code="https://changyeon.net/assets/data/2017.socc.dataset.tar.gz"
}

@InProceedings{Jo:2016:EDCS,
    abbr={EDCS'16},
    author = {Jo, Changyeon and Ahn, Changmin and Egger, Bernhard},
    title = {A Machine Learning-based Approach to Live Migration Modeling},
    booktitle = {4th International Workshop on Efficient Data Center Systems (EDCS'16)},
    series = {EDCS'16},
    year = {2016},
    location = {Seoul, Korea},
    abstract = "Live migration is one of the core technologies to increase the efficiency of data centers by enabling better power savings, a higher utilization, load balancing, and simplifying maintenance. With service-level agreements (SLA) in place, the overhead of live migration in terms of resources consumed on the host plus the performance reduction and downtime of the migrated VM poses a major obstacle to effectively apply live migration. With various live migration algorithms available, an important question is then which of the algorithms can provide optimal performance while respecting the SLAs. In this work, we propose a versatile model that is able to accurately predict the key metrics of live migration. The machine-learned model is trained with data from over 10,000 VM migrations and evaluated for the five live migration algorithms available in the latest QEMU/KVM virtualization environment. The evaluation shows that the proposed model is able to predict the total migration time and the total transferred data with over 90\% accuracy, and 90th percentile error of the downtime is 280ms.",
    pdf="publications/2016.EDCS.Jo.pdf"
}

@ARTICLE{Egger:2016:TC,
    abbr={TC'16},
    author={Bernhard Egger and Younghyun Cho and Changyeon Jo and Eunbyung Park and Jaejin Lee},
    journal={IEEE Transactions on Computers (TC'16)},
    title={Efficient Checkpointing of Live Virtual Machines},
    year={2016},
    volume={65},
    number={10},
    pages={3041-3054},
    keywords={Linux;cache storage;checkpointing;paged storage;virtual machines;virtualisation;I/O operations;Linux guests;PV guest;PVHVM;VM checkpoint storage;Windows guests;Xen hypervisor;checkpoint image;fully-virtualized guests;guest memory sizes;live virtual machine checkpointing;memory page mapping;operating system cache disk blocks;para-virtualized guests;space-efficient VM checkpointing;space-overhead;time-overhead;Checkpointing;Image restoration;Memory management;Operating systems;Virtual machine monitors;Virtual machining;Virtualization;Checkpointing;Virtualization;Xen},
    doi={10.1109/TC.2016.2519890},
    ISSN={0018-9340},
    month={Oct},
    abstract = "The ability to save the state of a running virtual machine (VM) for later restoration is an important tool for home, server, and virtual desktop cloud (VDC) environments in order to achieve optimal and balanced hardware utilization. With guest memory sizes of four to eight gigabytes being the norm the time- and space-overhead of storing VM checkpoints still prevents an effective use of the technique. This work presents a method for fast and space-efficient checkpointing of VMs. Based on the observation that operating systems cache disk blocks in memory, the proposed technique transparently intercepts I/O operations and maintains an up-to-date mapping of memory pages and disk blocks containing identical data. At a checkpoint, those memory pages are excluded from the checkpoint image leading to a significant reduction of both the time and space required to take a checkpoint of a running VM. The broad applicability and good performance of the proposed method is demonstrated by an extensive set of experiments. We have implemented the technique for para-virtualized (PV), PVHVM, and fully-virtualized (HVM) guests in the Xen hypervisor. In comparison with an unmodified Xen hypervisor, experiments with Linux and Windows guests, on average, achieve a 86\%, 76\%, 53\%, and 47\% reduction in the stored data and a 73\%, 62\%, 47\%, and 38\% shorter time required to take a checkpoint for PV, PVHVM, HVM Linux, and HVM Windows guests, respectively.",
    pdf="publications/2016.TC.Egger.pdf"
}

@article{Egger:2015:IJPP,
    abbr={IJPP'15},
    year={2015},
    issn={0885-7458},
    journal={International Journal of Parallel Programming (IJPP'15)},
    volume={43},
    number={3},
    doi={10.1007/s10766-013-0295-0},
    title={Efficiently Restoring Virtual Machines},
    url={http://dx.doi.org/10.1007/s10766-013-0295-0},
    publisher={Springer US},
    keywords={Virtualization; Checkpointing; Performance},
    author={Egger, Bernhard and Gustafsson, Erik and Jo, Changyeon and Son, Jeongseok},
    pages={421-439},
    language={English},
    abstract = "Saving the state of a running virtual machine (VM) for later restoration has become an indispensable tool to achieve balanced and energy-efficient usage of the underlying hardware in virtual desktop cloud environments (VDC). To free up resources, a remote user’s VM is saved to external storage when the user disconnects and restored when the user reconnects to the VDC. Existing techniques are able to reduce the size of the checkpoint image by up to 80 \% by excluding duplicated memory pages; however, those techniques suffer from a significantly increased restoration time which adversely affects the deployment of the technique in VDC environments. In this paper, we introduce a method to efficiently restore VMs from such space-optimized checkpoint images. With the presented method, a VM is available to the user before the entire memory contents of the VM have been restored. Using a combination of lazyfetch and intercepting accesses to yet unrestored pages we are able to reduce the timeto-responsiveness (TTR) for restored VMs to a few seconds. Experiments with VMs with 4 GB of memory running a wide range of benchmarks show that the proposed technique, on average, reduces the TTR by 50 \% compared to the Xen hypervisor. Compared to the previously fasted restoration of space-optimized checkpoints, the proposed technique achieves a threefold speedup on average.",
    pdf="publications/2015.IJPP.Egger.pdf"
}

@InProceedings{Jo:2013:CloudCom,
    abbr={CloudCom'13},
    author={Changyeon Jo and Bernhard Egger},
    booktitle={IEEE 5th International Conference on Cloud Computing Technology and Science (CloudCom'13)},
    title={Optimizing Live Migration for Virtual Desktop Clouds},
    series={CloudCom '13},
    year={2013},
    month={Dec},
    volume={1},
    pages={104-111},
    location={Bristol, UK},
    keywords={cloud computing;software performance evaluation;storage management;virtual machines;VDC;VM;Xen hypervisor;live migration optimization;memory pages;nonvolatile storage;performance analysis;physical host;precopy method;virtual desktop clouds;virtual machines;Benchmark testing;Memory management;Optimized production technology;Synchronization;Virtual machine monitors;live migration;shared storage;virtual desktop cloud},
    doi={10.1109/CloudCom.2013.21},
    abstract = "Live migration of virtual machines (VM) from one physical host to another is a key enabler for virtual desktop clouds (VDC). The prevalent algorithm, pre-copy, suffers from long migration times and a high data transfer volume for nonidle VMs which hinders effective use of live migration in VDC environments. In this paper, we present an optimization to the pre-copy method which is able to cut the total migration time in half. The key idea is to load memory pages duplicated on non-volatile storage directly and in parallel from the attached storage device. To keep the downtime short, outstanding data is fetched by a background process after the VM has been restarted on the target host. The proposed method has been implemented in the Xen hypervisor. A thorough performance analysis of the technique demonstrates that the proposed method significantly improves the performance of live migration: the total migration time is reduced up to 90\% for certain benchmarks and by 50\% on average at an equal or shorter downtime of the migrated VM with no or only minimal side-effects on co-located VMs.",
    pdf="publications/2013.CloudCom.Jo.pdf"
}

@InProceedings{Jo:2013:ELM:2451512.2451524,
    abbr={VEE'13},
    author = {Jo, Changyeon and Gustafsson, Erik and Son, Jeongseok and Egger, Bernhard},
    title = {Efficient live migration of virtual machines using shared storage},
    booktitle = {Proceedings of the 9th ACM SIGPLAN/SIGOPS international conference on Virtual Execution Environments (VEE'13)},
    series = {VEE '13},
    year = {2013},
    isbn = {978-1-4503-1266-0},
    location = {Houston, Texas, USA},
    pages = {41--50},
    numpages = {10},
    url = {http://doi.acm.org/10.1145/2451512.2451524},
    doi = {10.1145/2451512.2451524},
    acmid = {2451524},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {live migration, storage, virtual machine, xen},
    abstract = "Live migration of virtual machines (VM) across distinct physical hosts is an important feature of virtualization technology for maintenance, load-balancing and energy reduction, especially so for data centers operators and cluster service providers. Several techniques have been proposed to reduce the downtime of the VM being transferred, often at the expense of the total migration time. In this work, we present a technique to reduce the total time required to migrate a running VM from one host to another while keeping the downtime to a minimum. Based on the observation that modern operating systems use the better part of the physical memory to cache data from secondary storage, our technique tracks the VM’s I/O operations to the network-attached storage device and maintains an updated mapping of memory pages that currently reside in identical form on the storage device. During the iterative pre-copy live migration process, instead of transferring those pages from the source to the target host, the memory-to-disk mapping is sent to the target host which then fetches the contents directly from the network-attached storage device. We have implemented our approach into the Xen hypervisor and ran a series of experiments with Linux HVM guests. On average, the presented technique shows a reduction of up over 30\% on average of the total transfer time for a series of benchmarks.",
    pdf="publications/2013.VEE.Jo.pdf"
}

@InProceedings{Jeong:2012:MTV,
    abbr={MTV'12},
    author = {Jeong, Seonghun and Cho, Youngchul and Shin, Daeyong and Jo, Changyeon and Han, Yenjo and Ryu, Soojung and Kim, Jeongwook and Egger, Bernhard},
    title = {Random Test Program Generation for Reconfigurable Architectures},
    booktitle={13th International Workshop on Microprocessor Test and Verification (MTV'12)},
    year = {2012},
    location = {Austin, Texas, USA},
    keywords = {random test generation, verification},
    abstract = "Automatic generation of test programs plays a major role in the verification of microprocessors. In this work, we propose a random test program generator (RTPG) framework for reconfigurable architectures. Reconfigurable architectures pose a number of problems to existing RTPGs. The proposed framework overcomes these problems by building a hardware model directly from the architecture description. Our RTPG only tracks the type of the data values. This enables the framework to seamlessly support custom ISA extensions whose semantics are not available to the RTPG. We implement the proposed RTPG framework for the Samsung Reconfigurable Processor (SRP), a low-power high-performance reconfigurable architecture consisting of a VLIW and a coarse-grained reconfigurable array processor. The experiments show that the framework is flexible, efficient and quickly achieves a high coverage in the generated test programs.",
    pdf="publications/2012.MTV.Jeong.pdf"
}
