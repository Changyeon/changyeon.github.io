<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="_f1LXQVz6NNIP3dSCVXfNNYeWf7MNkp31jVlFtXdjpw" />

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Changyeon  Jo | publications</title>
    <meta name="author" content="Changyeon  Jo" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÉ</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://changyeon.net/publications/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://changyeon.net/"><span class="font-weight-bold">Changyeon</span>   Jo</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PPoPP‚Äô22</abbr></div>

        <!-- Entry bib key -->
        <div id="Cho:2022:PPoPP" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Dopia: Online Parallelism Management for Integrated CPU/GPU Architectures</div>
          <!-- Author -->
          <div class="author">Younghyun Cho,¬†Jiyeon Park,¬†Florian Negele,¬†
                  <em>Changyeon Jo</em>,¬†Thomas R. Gross,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 2022)</em> Apr 2022
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2022.PPoPP.Cho.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent desktop and mobile processors often integrate CPU and GPU onto the same die. The limited memory bandwidth of these integrated architectures can negatively affect the performance of data-parallel workloads when all computational resources are active. The combination of active CPU and GPU cores achieving the maximum performance depends on a workload‚Äôs characteristics, making manual tuning a time-consuming task. Dopia is a fully automated framework that improves the performance of data-parallel workloads by adjusting the Degree Of Parallelism on Integrated Architectures. Dopia transparently analyzes and rewrites OpenCL kernels before executing them with the number of CPU and GPU cores expected to yield the best performance. Evaluated on AMD and Intel integrated processors, Dopia achieves 84% of the maximum performance attainable by an oracle.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">GECON‚Äô21</abbr></div>

        <!-- Entry bib key -->
        <div id="Kim:2021:GECON" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">RapidSwap: a Hierarchical Far Memory</div>
          <!-- Author -->
          <div class="author">Hyunik Kim,¬†
                  <em>Changyeon Jo</em>,¬†J√∂rn Altmann,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 18th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON‚Äô21)</em> Apr 2021
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2021.GECON.Kim.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As more and more memory-intensive applications are moved into the cloud, data center operators face the challenge of providing sufficient main memory resources while achieving high resource utilization. Solutions to overcome the unsatisfying performance degradation of traditional on-demand paging include memory disaggregation that allows applications to access remote memory or compressing memory pages in local DRAM; however, the former‚Äôs extended failure domain and the latter‚Äôs low efficacy limit their broad applicability. This paper presents RapidSwap, a hierarchical far memory manager that exploits the wide availability of phase-change memory (Intel Optane memory) in data centers to achieve quasi-DRAM performance at a significantly lower total cost of ownership (TCO). RapidSwap migrates infrequently accessed data to slower and cheaper devices in a hierarchy of storage devices by tracking applications‚Äô memory accesses. Evaluated with several real-world cloud benchmarks, RapidSwap achieves a reduction of 20% in operating cost at minimal performance degradation and is 30% more cost-effective than pure DRAM solutions. The results demonstrate that proper management of new memory technologies can yield significant TCO savings in cloud data centers.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">GECON‚Äô21</abbr></div>

        <!-- Entry bib key -->
        <div id="Park:2021:GECON" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Can VM Live Migration Improve Job Throughput? Evidence from a Real World Cluster Trace</div>
          <!-- Author -->
          <div class="author">Daon Park,¬†Hyunsoo Kim,¬†Youngsu Cho,¬†
                  <em>Changyeon Jo</em>,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 18th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON‚Äô21)</em> Apr 2021
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2021.GECON.Park.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Cloud resource providers are putting more and more emphasis on efficiently management the resources of their data centers to achieve high utilization while minimizing energy consumption. Despite these efforts, an analysis of recent data center traces reveals that the utilization of CPU and memory resources has not improved significantly over the past decade. Resource overcommitment is a promising approach to improve resource utilization, because most workloads show a significant gap between their guaranteed and actually consumed resources. A wrong prediction of the actual usage, however, can lead to a severe performance degradation on overloaded nodes. Combining resource overcommitment with live migration of tasks can alleviate the situation, but its prohibitively high cost has so far prevented a wide adoption. Recent and rapid advancements in networking technology, however, are changing the status quo. With throughputs surpassing 100ÌòªGb/s in 2021, even large tasks can be migrated within a few seconds. In light of these improvements, we believe it is time to rethink the application of resource overcommitment and live migration to improve data center resource utilization. Based on real-world cluster traces published by Google in 2019, we show that combining resource overcommitment with task live migration can reduce the mean task turnaround time by 16%, demonstrating that further research in this direction is both warranted and promising.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PACT‚Äô20</abbr></div>

        <!-- Entry bib key -->
        <div id="Jo:2020:PACT:RackMem" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">RackMem: A Tailored Caching Layer for Rack Scale Computing</div>
          <!-- Author -->
          <div class="author">
                  <em>Changyeon Jo</em>,¬†Hyunik Kim,¬†Hexiang Geng,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2020 International Conference on Parallel Architectures and Compilation (PACT‚Äô20)</em> Apr 2020
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2020.PACT.Jo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>High-performance computing (HPC) clusters suffer from an overall low memory utilization that is caused by the node-centric memory allocation combined with the variable memory requirements of HPC workloads. The recent provisioning of nodes with terabytes of memory to accommodate workloads with extreme peak memory requirements further exacerbates the problem. Memory disaggregation is viewed as a promising remedy to increase overall resource utilization and enable cost-effective up-scaling and efficient operation of HPC clusters, however, the overhead of demand paging in virtual memory management has so far hindered performant implementations. To overcome these limitations, this work presents RackMem, an efficient implementation of disaggregated memory for rack scale computing. RackMem addresses the shortcomings of Linux‚Äôs demand paging algorithm and automatically adapts to the memory access patterns of individual processes to minimize the inherent overhead of remote memory accesses. Evaluated on a cluster with an Infiniband interconnect, RackMem outperforms the state-of-the-art RDMA implementation and Linux‚Äôs virtual memory paging by a significant margin. RackMem‚Äôs custom demand paging implementation achieves a tail latency that is two orders of magnitude better than that of the Linux kernel. Compared to the state-of-the-art remote paging solution, RackMem achieves a 28% higher throughput and a 44% lower tail latency for a wide variety of real-world workloads.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">GECON‚Äô20</abbr></div>

        <!-- Entry bib key -->
        <div id="Jo:2020:GECON" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Instant Virtual Machine Live Migration</div>
          <!-- Author -->
          <div class="author">
                  <em>Changyeon Jo</em>,¬†Hyunik Kim,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 17th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON‚Äô20)</em> Apr 2020
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2020.GECON.Jo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Live migration of virtual machines (VMs) is an important tool for data center operators to achieve maintenance, power management, and load balancing. The relatively high cost of live migration makes it difficult to employ live migration for rapid load balancing or power management operations, leaving much of its promised benefits unused. The advance of fast network interconnects has led to the development of distributed shared memory (DSM) that allows a cluster of nodes to utilize remote memory with relatively low overhead. In this paper, we explore VM live migration over DSM. We present and evaluate a novel live migration algorithm on our own DSM implementation. The evaluation of live migrating various VMs executing real-life workloads shows that live migration over DSM can reduce the total migration time by 70% and the total amount of transferred data by 65% on average. The absolute average total migration time of only 1.1 seconds demonstrates the potential of live migration over DSM to lead to better load balancing, energy management, and total cost of ownership</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">GECON‚Äô20</abbr></div>

        <!-- Entry bib key -->
        <div id="Cho:2020:GECON" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Towards Economical Live Migration in Data Centers</div>
          <!-- Author -->
          <div class="author">Youngsu Cho,¬†
                  <em>Changyeon Jo</em>,¬†Hyunik Kim,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 17th International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON‚Äô20)</em> Apr 2020
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2020.GECON.Cho.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Live migration of virtual machines (VMs) enables maintenance, load balancing, and power management in data centers. The cost of live migration on several key metrics combined with strict service-level objectives (SLOs), however, typically limits its practical application to situations where the underlying physical host has to undergo maintenance. As a consequence, the potential benefits of live migration with respect to increased resource usage and lower power consumption remain largely untouched. In this paper, we argue that live migration-aware SLOs combined with smart live migration algorithm selection provides an economically viable model for live migration in data centers. Based on a model predicting key parameters of VM live migration, an optimization algorithm selects the live migration technique that is expected to meet client SLOs while at the same time to optimize target metrics given by the data center operator. A comparison with the state-of-the-art shows that the presented guided live migration technique selection achieves significantly fewer SLO violations while, at the same time, minimizing the effect of live migration on the infrastructure</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">SoCC‚Äô17</abbr></div>

        <!-- Entry bib key -->
        <div id="Jo:2017:SoCC" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Machine Learning Approach to Live Migration Modeling</div>
          <!-- Author -->
          <div class="author">
                  <em>Changyeon Jo</em>,¬†Youngsu Cho,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ACM Symposium on Cloud Computing (SoCC‚Äô17)</em> Sep 2017
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2017.SoCC.Jo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://changyeon.net/assets/data/2017.socc.dataset.tar.gz" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Live migration is one of the key technologies to improve data center utilization, power efficiency, and maintenance. Various live migration algorithms have been proposed; each exhibiting distinct characteristics in terms of completion time, amount of data transferred, virtual machine (VM) downtime, and VM performance degradation. To make matters worse, not only the migration algorithm but also the applications running inside the migrated VM affect the different performance metrics. With service-level agreements and operational constraints in place, choosing the optimal live migration technique has so far been an open question. In this work, we propose an adaptive machine learning-based model that is able to predict with high accuracy the key characteristics of live migration in dependence of the migration algorithm and the workload running inside the VM. We discuss the important input parameters for accurately modeling the target metrics, and describe how to profile them with little overhead. Compared to existing work, we are not only able to model all commonly used migration algorithms but also predict important metrics that have not been considered so far such as the performance degradation of the VM. In a comparison with the state-of-the-art, we show that the proposed model outperforms existing work by a factor 2 to 5.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EDCS‚Äô16</abbr></div>

        <!-- Entry bib key -->
        <div id="Jo:2016:EDCS" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Machine Learning-based Approach to Live Migration Modeling</div>
          <!-- Author -->
          <div class="author">
                  <em>Changyeon Jo</em>,¬†Changmin Ahn,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 4th International Workshop on Efficient Data Center Systems (EDCS‚Äô16)</em> Sep 2016
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2016.EDCS.Jo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Live migration is one of the core technologies to increase the efficiency of data centers by enabling better power savings, a higher utilization, load balancing, and simplifying maintenance. With service-level agreements (SLA) in place, the overhead of live migration in terms of resources consumed on the host plus the performance reduction and downtime of the migrated VM poses a major obstacle to effectively apply live migration. With various live migration algorithms available, an important question is then which of the algorithms can provide optimal performance while respecting the SLAs. In this work, we propose a versatile model that is able to accurately predict the key metrics of live migration. The machine-learned model is trained with data from over 10,000 VM migrations and evaluated for the five live migration algorithms available in the latest QEMU/KVM virtualization environment. The evaluation shows that the proposed model is able to predict the total migration time and the total transferred data with over 90% accuracy, and 90th percentile error of the downtime is 280ms.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TC‚Äô16</abbr></div>

        <!-- Entry bib key -->
        <div id="Egger:2016:TC" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Efficient Checkpointing of Live Virtual Machines</div>
          <!-- Author -->
          <div class="author">Bernhard Egger,¬†Younghyun Cho,¬†
                  <em>Changyeon Jo</em>,¬†Eunbyung Park,¬†and Jaejin Lee
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Computers (TC‚Äô16)</em> Oct 2016
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2016.TC.Egger.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The ability to save the state of a running virtual machine (VM) for later restoration is an important tool for home, server, and virtual desktop cloud (VDC) environments in order to achieve optimal and balanced hardware utilization. With guest memory sizes of four to eight gigabytes being the norm the time- and space-overhead of storing VM checkpoints still prevents an effective use of the technique. This work presents a method for fast and space-efficient checkpointing of VMs. Based on the observation that operating systems cache disk blocks in memory, the proposed technique transparently intercepts I/O operations and maintains an up-to-date mapping of memory pages and disk blocks containing identical data. At a checkpoint, those memory pages are excluded from the checkpoint image leading to a significant reduction of both the time and space required to take a checkpoint of a running VM. The broad applicability and good performance of the proposed method is demonstrated by an extensive set of experiments. We have implemented the technique for para-virtualized (PV), PVHVM, and fully-virtualized (HVM) guests in the Xen hypervisor. In comparison with an unmodified Xen hypervisor, experiments with Linux and Windows guests, on average, achieve a 86%, 76%, 53%, and 47% reduction in the stored data and a 73%, 62%, 47%, and 38% shorter time required to take a checkpoint for PV, PVHVM, HVM Linux, and HVM Windows guests, respectively.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IJPP‚Äô15</abbr></div>

        <!-- Entry bib key -->
        <div id="Egger:2015:IJPP" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Efficiently Restoring Virtual Machines</div>
          <!-- Author -->
          <div class="author">Bernhard Egger,¬†Erik Gustafsson,¬†
                  <em>Changyeon Jo</em>,¬†and Jeongseok Son
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Journal of Parallel Programming (IJPP‚Äô15)</em> Oct 2015
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2015.IJPP.Egger.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Saving the state of a running virtual machine (VM) for later restoration has become an indispensable tool to achieve balanced and energy-efficient usage of the underlying hardware in virtual desktop cloud environments (VDC). To free up resources, a remote user‚Äôs VM is saved to external storage when the user disconnects and restored when the user reconnects to the VDC. Existing techniques are able to reduce the size of the checkpoint image by up to 80 % by excluding duplicated memory pages; however, those techniques suffer from a significantly increased restoration time which adversely affects the deployment of the technique in VDC environments. In this paper, we introduce a method to efficiently restore VMs from such space-optimized checkpoint images. With the presented method, a VM is available to the user before the entire memory contents of the VM have been restored. Using a combination of lazyfetch and intercepting accesses to yet unrestored pages we are able to reduce the timeto-responsiveness (TTR) for restored VMs to a few seconds. Experiments with VMs with 4 GB of memory running a wide range of benchmarks show that the proposed technique, on average, reduces the TTR by 50 % compared to the Xen hypervisor. Compared to the previously fasted restoration of space-optimized checkpoints, the proposed technique achieves a threefold speedup on average.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2013</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CloudCom‚Äô13</abbr></div>

        <!-- Entry bib key -->
        <div id="Jo:2013:CloudCom" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Optimizing Live Migration for Virtual Desktop Clouds</div>
          <!-- Author -->
          <div class="author">
                  <em>Changyeon Jo</em>,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE 5th International Conference on Cloud Computing Technology and Science (CloudCom‚Äô13)</em> Dec 2013
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2013.CloudCom.Jo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Live migration of virtual machines (VM) from one physical host to another is a key enabler for virtual desktop clouds (VDC). The prevalent algorithm, pre-copy, suffers from long migration times and a high data transfer volume for nonidle VMs which hinders effective use of live migration in VDC environments. In this paper, we present an optimization to the pre-copy method which is able to cut the total migration time in half. The key idea is to load memory pages duplicated on non-volatile storage directly and in parallel from the attached storage device. To keep the downtime short, outstanding data is fetched by a background process after the VM has been restarted on the target host. The proposed method has been implemented in the Xen hypervisor. A thorough performance analysis of the technique demonstrates that the proposed method significantly improves the performance of live migration: the total migration time is reduced up to 90% for certain benchmarks and by 50% on average at an equal or shorter downtime of the migrated VM with no or only minimal side-effects on co-located VMs.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">VEE‚Äô13</abbr></div>

        <!-- Entry bib key -->
        <div id="Jo:2013:ELM:2451512.2451524" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Efficient live migration of virtual machines using shared storage</div>
          <!-- Author -->
          <div class="author">
                  <em>Changyeon Jo</em>,¬†Erik Gustafsson,¬†Jeongseok Son,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 9th ACM SIGPLAN/SIGOPS international conference on Virtual Execution Environments (VEE‚Äô13)</em> Dec 2013
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2013.VEE.Jo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Live migration of virtual machines (VM) across distinct physical hosts is an important feature of virtualization technology for maintenance, load-balancing and energy reduction, especially so for data centers operators and cluster service providers. Several techniques have been proposed to reduce the downtime of the VM being transferred, often at the expense of the total migration time. In this work, we present a technique to reduce the total time required to migrate a running VM from one host to another while keeping the downtime to a minimum. Based on the observation that modern operating systems use the better part of the physical memory to cache data from secondary storage, our technique tracks the VM‚Äôs I/O operations to the network-attached storage device and maintains an updated mapping of memory pages that currently reside in identical form on the storage device. During the iterative pre-copy live migration process, instead of transferring those pages from the source to the target host, the memory-to-disk mapping is sent to the target host which then fetches the contents directly from the network-attached storage device. We have implemented our approach into the Xen hypervisor and ran a series of experiments with Linux HVM guests. On average, the presented technique shows a reduction of up over 30% on average of the total transfer time for a series of benchmarks.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">MTV‚Äô12</abbr></div>

        <!-- Entry bib key -->
        <div id="Jeong:2012:MTV" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Random Test Program Generation for Reconfigurable Architectures</div>
          <!-- Author -->
          <div class="author">Seonghun Jeong,¬†Youngchul Cho,¬†Daeyong Shin,¬†
                  <em>Changyeon Jo</em>,¬†Yenjo Han,¬†Soojung Ryu,¬†Jeongwook Kim,¬†and Bernhard Egger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 13th International Workshop on Microprocessor Test and Verification (MTV‚Äô12)</em> Dec 2012
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/publications/2012.MTV.Jeong.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Automatic generation of test programs plays a major role in the verification of microprocessors. In this work, we propose a random test program generator (RTPG) framework for reconfigurable architectures. Reconfigurable architectures pose a number of problems to existing RTPGs. The proposed framework overcomes these problems by building a hardware model directly from the architecture description. Our RTPG only tracks the type of the data values. This enables the framework to seamlessly support custom ISA extensions whose semantics are not available to the RTPG. We implement the proposed RTPG framework for the Samsung Reconfigurable Processor (SRP), a low-power high-performance reconfigurable architecture consisting of a VLIW and a coarse-grained reconfigurable array processor. The experiments show that the framework is flexible, efficient and quickly achieves a high coverage in the generated test programs.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2023 Changyeon  Jo. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>
  </body>

  <!-- jQuery -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  <!-- Mansory & imagesLoaded -->
  <script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  <script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
  
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-62711617-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-62711617-1');
  </script>
</html>

